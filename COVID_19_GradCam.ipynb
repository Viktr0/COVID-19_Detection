{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of COVID-19 GradCam.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF9XHjHrW6wN"
      },
      "source": [
        "# Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dWOOEgGKQhC"
      },
      "source": [
        "# IMPORT NECESSARY LIBS\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import tqdm\n",
        "import copy\n",
        "from PIL import Image, ImageFilter\n",
        "import matplotlib.cm as mpl_color_map\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torchvision import models\n",
        "from torch.optim import Adam\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9tY_R8rOZwj",
        "outputId": "cf960029-2c11-4a92-fcf8-3f4ff00c0fc2"
      },
      "source": [
        "# MOUNT GOOGLE DRIVE\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJqr9HuUQXg6"
      },
      "source": [
        "# Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XW8kD7Qy0ABp"
      },
      "source": [
        "# VGG11 MODEL\n",
        "\n",
        "# VGG11\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, features, output_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.features = features\n",
        "        \n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(7)\n",
        "        \n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 4096),\n",
        "            nn.ReLU(inplace = True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace = True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, output_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        h = x.view(x.shape[0], -1)\n",
        "        x = self.classifier(h)\n",
        "        return x, h\n",
        "\n",
        "# Config for VGG11\n",
        "vgg11_config = [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n",
        "\n",
        "# Build the model from config file\n",
        "def get_vgg_layers(config, batch_norm):\n",
        "    \n",
        "    layers = []\n",
        "    in_channels = 3\n",
        "    \n",
        "    for c in config:\n",
        "        assert c == 'M' or isinstance(c, int)\n",
        "        if c == 'M':\n",
        "            layers += [nn.MaxPool2d(kernel_size = 2)]\n",
        "        else:\n",
        "            conv2d = nn.Conv2d(in_channels, c, kernel_size = 3, padding = 1)\n",
        "            if batch_norm:\n",
        "                layers += [conv2d, nn.BatchNorm2d(c), nn.ReLU(inplace = True)]\n",
        "            else:\n",
        "                layers += [conv2d, nn.ReLU(inplace = True)]\n",
        "            in_channels = c\n",
        "            \n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "vgg11_layers = get_vgg_layers(vgg11_config, batch_norm = True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGqdEatjQfXd"
      },
      "source": [
        "# Basic methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7nTDsCrPL2A"
      },
      "source": [
        "# SAVE IMAGE\n",
        "\n",
        "def save_image(im, path):\n",
        "  \n",
        "    if isinstance(im, (np.ndarray, np.generic)):\n",
        "        im = format_np_output(im)\n",
        "        im = Image.fromarray(im)\n",
        "    im.save(path)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mN51T4lPUR8"
      },
      "source": [
        "# PREPROCESS IMAGE\n",
        "\n",
        "def preprocess_image(pil_im, resize_im=True):\n",
        "\n",
        "    if type(pil_im) != Image.Image:\n",
        "        try:\n",
        "            pil_im = Image.fromarray(pil_im)\n",
        "        except Exception as e:\n",
        "            print(\"could not transform PIL_img to a PIL Image object. Please check input.\")\n",
        "\n",
        "    # Resize image\n",
        "    if resize_im:\n",
        "        pil_im = pil_im.resize((224, 224), Image.ANTIALIAS)\n",
        "\n",
        "    # Convert image to D,W,H array\n",
        "    im_as_arr = np.float32(pil_im)\n",
        "    im_as_arr = im_as_arr.transpose(2, 0, 1)\n",
        "    \n",
        "    # Normalize the channels\n",
        "    for channel, _ in enumerate(im_as_arr):\n",
        "        im_as_arr[channel] /= 255\n",
        "\n",
        "    im_as_ten = torch.from_numpy(im_as_arr).float()\n",
        "    im_as_ten.unsqueeze_(0)\n",
        "    im_as_var = Variable(im_as_ten, requires_grad=True)\n",
        "    \n",
        "    return im_as_var\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEjq2rnWPckR"
      },
      "source": [
        "# CONVERT TO 3xWxH format\n",
        "\n",
        "def format_np_output(np_arr):\n",
        "    \n",
        "    # Phase/Case 1: The np arr only has 2 dimensions\n",
        "    if len(np_arr.shape) == 2:\n",
        "        np_arr = np.expand_dims(np_arr, axis=0)\n",
        "\n",
        "    # Phase/Case 2: Np arr has only 1 channel (assuming first dim is channel)\n",
        "    if np_arr.shape[0] == 1:\n",
        "        np_arr = np.repeat(np_arr, 3, axis=0)\n",
        "\n",
        "    # Phase/Case 3: Np arr is of shape 3xWxH\n",
        "    if np_arr.shape[0] == 3:\n",
        "        np_arr = np_arr.transpose(1, 2, 0)\n",
        "\n",
        "    # Phase/Case 4: NP arr is normalized between 0-1\n",
        "    if np.max(np_arr) <= 1:\n",
        "        np_arr = (np_arr*255).astype(np.uint8)\n",
        "        \n",
        "    return np_arr"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C79fRWXyMToY"
      },
      "source": [
        "# GET EXAMPLES, SET PATHES, LOAD BEST WEIGHTS\n",
        "\n",
        "def get_example_params(example_index):\n",
        "\n",
        "    # 0 - covid; 1 - normal\n",
        "    root_path = '/content/drive/MyDrive/Egyetem/Szakdolgozat/Code/Datasets/'\n",
        "    root_path = root_path + 'CXR/' if cxr is True else root_path + 'CT/'\n",
        "    root_path = root_path + 'test/'\n",
        "    covid_path = root_path + 'covid-19/'\n",
        "    normal_path = root_path + 'normal/'\n",
        "\n",
        "    # CXR database\n",
        "    example_list_cxr = (('/content/drive/MyDrive/Egyetem/Szakdolgozat/Code/Datasets/CXR/test/normal/normal_913.png', 1),\n",
        "                        ('/content/drive/MyDrive/Egyetem/Szakdolgozat/Code/Datasets/CXR/test/covid/covid_889.png', 0),\n",
        "                        ('/content/drive/MyDrive/Egyetem/Szakdolgozat/Code/Datasets/CXR/test/covid/covid_906.png', 0),\n",
        "                        ('/content/drive/MyDrive/Egyetem/Szakdolgozat/Code/Datasets/CXR/test/normal/normal_894.png', 1),\n",
        "                        ('/content/drive/MyDrive/Egyetem/Szakdolgozat/Code/Datasets/CXR/test/normal/normal_966.png', 1))\n",
        "    \n",
        "    # CT database\n",
        "    example_list_ct = (('/content/drive/MyDrive/Egyetem/Szakdolgozat/Code/Datasets/CT/test/covid/covid_322.png', 0),\n",
        "                       ('/content/drive/MyDrive/Egyetem/Szakdolgozat/Code/Datasets/CT/test/covid/covid_328.png', 0),\n",
        "                    ('/content/drive/MyDrive/Egyetem/Szakdolgozat/Code/Datasets/CT/test/covid/covid_344.png', 0),\n",
        "                    ('/content/drive/MyDrive/Egyetem/Szakdolgozat/Code/Datasets/CT/test/normal/normal_361.png', 1),\n",
        "                    ('/content/drive/MyDrive/Egyetem/Szakdolgozat/Code/Datasets/CT/test/normal/normal_386.png', 1))\n",
        "    \n",
        "    example_list = example_list_cxr if cxr is True else example_list_ct\n",
        "    img_path = example_list[example_index][0]\n",
        "    target_class = example_list[example_index][1]\n",
        "    file_name_to_export = img_path[img_path.rfind('/')+1:img_path.rfind('.')]\n",
        "    \n",
        "    # Read image\n",
        "    original_image = Image.open(img_path).convert('RGB')\n",
        "    original_image = original_image.resize((224, 224), Image.ANTIALIAS)\n",
        "\n",
        "    \n",
        "    # Process image\n",
        "    prep_img = preprocess_image(original_image)\n",
        "    \n",
        "    # Define model\n",
        "    OUTPUT_DIM = 2\n",
        "    pretrained_model = VGG(vgg11_layers, OUTPUT_DIM)\n",
        "    path_cxr = \"/content/drive/MyDrive/Egyetem/Szakdolgozat/Code/Classification/checkpoints/best_cxr_classifier_0_99.pt\"\n",
        "    path_ct = '/content/drive/MyDrive/Egyetem/Szakdolgozat/Code/Classification/cp_teszt/best_ct_classifier_5_81.pt'\n",
        "    checkpoint = torch.load(path_ct, map_location ='cpu')\n",
        "    pretrained_model.load_state_dict(checkpoint)\n",
        "\n",
        "    return (original_image,\n",
        "            prep_img,\n",
        "            target_class,\n",
        "            file_name_to_export,\n",
        "            pretrained_model)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9B9HwDdMjmw"
      },
      "source": [
        "# SAVE IMAGE AND ACTIVATION HEATMAP ON IMAGE\n",
        "\n",
        "def save_class_activation_images(org_img, activation_map, file_name, tl, first=False):\n",
        "\n",
        "    # Set pathes\n",
        "    result_path = '/content/drive/MyDrive/Egyetem/Szakdolgozat/Code/GradCam/results/'\n",
        "    if not os.path.exists(result_path):\n",
        "        os.makedirs(result_path)\n",
        "    result_path = result_path + 'CXR/' if cxr is True else result_path + 'CT/'\n",
        "    result_path = result_path + 'covid' if target_class is 0 else result_path + 'normal'\n",
        "\n",
        "    heatmap, heatmap_on_image = apply_colormap_on_image(org_img, activation_map, 'hsv')\n",
        "    \n",
        "    # Save original image\n",
        "    path_to_file = os.path.join(result_path, file_name+'.png')\n",
        "    if first:\n",
        "        save_image(org_img, path_to_file)\n",
        "\n",
        "    # Save heatmap on iamge\n",
        "    path_to_file = os.path.join(result_path, file_name+tl+'_Cam_On_Image.png')\n",
        "    save_image(heatmap_on_image, path_to_file)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq47eAKbMkX2"
      },
      "source": [
        "# APPLY HEATMAP ON IMAGE\n",
        "\n",
        "def apply_colormap_on_image(org_im, activation, colormap_name):\n",
        "    \n",
        "    # Get colormap\n",
        "    color_map = mpl_color_map.get_cmap(colormap_name)\n",
        "    no_trans_heatmap = color_map(activation)\n",
        "\n",
        "    heatmap = copy.copy(no_trans_heatmap)\n",
        "    heatmap[:, :, 3] = 0.4\n",
        "    heatmap = Image.fromarray((heatmap*255).astype(np.uint8))\n",
        "    no_trans_heatmap = Image.fromarray((no_trans_heatmap*255).astype(np.uint8))\n",
        "\n",
        "    heatmap_on_image = org_im\n",
        "    heatmap_on_image = heatmap_on_image.convert('RGBA')\n",
        "    heatmap_on_image = Image.alpha_composite(heatmap_on_image, heatmap)\n",
        "    return no_trans_heatmap, heatmap_on_image"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmNfwqxYWhqS"
      },
      "source": [
        "# Produce CAM (Class Activation Map)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKSRFhiXr0pS"
      },
      "source": [
        "# EXTRACT CAM FEATURES FROM THE MODEL\n",
        "\n",
        "class CamExtractor():\n",
        "    def __init__(self, model, target_layer):\n",
        "        self.model = model\n",
        "        self.target_layer = target_layer\n",
        "        self.gradients = None\n",
        "\n",
        "    def save_gradient(self, grad):\n",
        "        self.gradients = grad\n",
        "\n",
        "    # Save the convolution output on that layer\n",
        "    def forward_pass_on_convolutions(self, x):\n",
        "        conv_output = None\n",
        "        for module_pos, module in self.model.features._modules.items():\n",
        "            x = module(x)\n",
        "            if int(module_pos) == self.target_layer:\n",
        "                x.register_hook(self.save_gradient)\n",
        "                conv_output = x\n",
        "        return conv_output, x\n",
        "\n",
        "    # Forward pass on the convolutions\n",
        "    def forward_pass(self, x):\n",
        "        conv_output, x = self.forward_pass_on_convolutions(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.model.classifier(x)\n",
        "        return conv_output, x"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oncO36YjNSBm"
      },
      "source": [
        "# GENERATE CLASS ACTIVATION MAP\n",
        "\n",
        "class GradCam():\n",
        "\n",
        "    def __init__(self, model, target_layer):\n",
        "        self.model = model\n",
        "        self.model.eval()\n",
        "        self.extractor = CamExtractor(self.model, target_layer)\n",
        "\n",
        "    def generate_cam(self, input_image, target_class=None):\n",
        "\n",
        "        # conv_output is the output of convolutions at specified layer\n",
        "        # model_output is the final output of the model\n",
        "        conv_output, model_output = self.extractor.forward_pass(input_image)\n",
        "        if target_class is None:\n",
        "            target_class = np.argmax(model_output.data.numpy())\n",
        "        \n",
        "        # Target for backprop\n",
        "        one_hot_output = torch.FloatTensor(1, model_output.size()[-1]).zero_()\n",
        "        one_hot_output[0][target_class] = 1\n",
        "        # Zero grads\n",
        "        self.model.features.zero_grad()\n",
        "        self.model.classifier.zero_grad()\n",
        "        \n",
        "        # Backward pass with specified target\n",
        "        model_output.backward(gradient=one_hot_output, retain_graph=True)\n",
        "        # Get hooked gradients\n",
        "        guided_gradients = self.extractor.gradients.data.numpy()[0]\n",
        "        # Get convolution outputs\n",
        "        target = conv_output.data.numpy()[0]\n",
        "        # Get weights from gradients\n",
        "        weights = np.mean(guided_gradients, axis=(1, 2))\n",
        "        cam = np.ones(target.shape[1:], dtype=np.float32)\n",
        "\n",
        "        # Multiply each weight with its conv output and then sum\n",
        "        for i, w in enumerate(weights):\n",
        "            cam += w * target[i, :, :]\n",
        "        cam = np.maximum(cam, 0)\n",
        "        # Normalize between 0-1\n",
        "        cam = (cam - np.min(cam)) / (np.max(cam) - np.min(cam))\n",
        "        # Scale between 0-255 to visualize\n",
        "        cam = np.uint8(cam * 255)\n",
        "        cam = np.uint8(Image.fromarray(cam).resize((input_image.shape[2],\n",
        "                       input_image.shape[3]), Image.ANTIALIAS))/255\n",
        "\n",
        "        return cam"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxUCffvIWymY"
      },
      "source": [
        "# Execute"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWxnfEeuNTWL"
      },
      "source": [
        "    # Get params\n",
        "    cxr = False\n",
        "    target_example = 0\n",
        "    (original_image, prep_img, target_class, file_name_to_export, pretrained_model) =\\\n",
        "        get_example_params(target_example)\n",
        "    \n",
        "    # Conv2d layers: 0, 4, 8, 11, 15, 18, 22, 25\n",
        "    conv_layers = [4, 8, 11, 15, 18, 22, 25]\n",
        "    for target_layer in conv_layers:\n",
        "        \n",
        "        # Set local params\n",
        "        isFirst = True if target_layer == conv_layers[0] else False\n",
        "        layer_id = \"_\" + str(target_layer)\n",
        "        \n",
        "        # Init grad cam and generate mask\n",
        "        grad_cam = GradCam(pretrained_model, target_layer=target_layer)\n",
        "        cam = grad_cam.generate_cam(prep_img, target_class)\n",
        "\n",
        "        # Save mask\n",
        "        save_class_activation_images(original_image, cam, file_name_to_export, layer_id, isFirst)"
      ],
      "execution_count": 18,
      "outputs": []
    }
  ]
}